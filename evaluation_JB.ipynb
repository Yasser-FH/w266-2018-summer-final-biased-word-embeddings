{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import debiaswe as dwe\n",
    "import debiaswe.we as we\n",
    "from debiaswe.we import WordEmbedding\n",
    "from debiaswe.debias import debias\n",
    "import vector_math_jb as vm \n",
    "\n",
    "# Standard python helper libraries.\n",
    "import os, sys, re, json, time\n",
    "import itertools, collections\n",
    "#from importlib import reload\n",
    "from IPython.display import display\n",
    "\n",
    "# NumPy and SciPy for matrix ops\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK for NLP utils\n",
    "import nltk\n",
    "\n",
    "# PCA \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary#, tf_embed_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from ./embeddings/w2v_gnews_small.txt\n",
      "(26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n",
      "*** Reading data from ./embeddings/RANE_300d_english_50k.txt\n",
      "(50000, 300)\n",
      "50000 words of dimension 300 : the, to, of, and, ..., prin, synchronise, dissertations, mammography\n",
      "50000 words of dimension 300 : the, to, of, and, ..., prin, synchronise, dissertations, mammography\n",
      "*** Reading data from ./embeddings/w2v_gnews_small_debiased.txt\n",
      "(26423, 300)\n",
      "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n"
     ]
    }
   ],
   "source": [
    "# load subset of word embedding trained on Google News text\n",
    "E_gn = WordEmbedding(\"./embeddings/w2v_gnews_small.txt\")\n",
    "E_jp = WordEmbedding(\"./embeddings/RANE_300d_english_50k.txt\")\n",
    "E_gn_db = WordEmbedding(\"./embeddings/w2v_gnews_small_debiased.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "File #                     Embeddings      RG (53/65)    WS (318/353)\n",
      "========================================================================\n",
      "     1                  article_embed          0.5378          0.5702 \n",
      "\n",
      "     2          RANE_300d_english_50k          0.6052          0.6159 \n",
      "\n",
      "     3                w2v_gnews_small          0.7618          0.6857 \n",
      "\n",
      "     4       w2v_gnews_small_debiased          0.7601          0.6826 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adapted from Faruqui and Dyer, Community Evaluation and Exchange of Word Vectors at wordvectors.org\n",
    "# RG = Rubenstein and Goodenough, 1965; WS = Finkelstein et. al, 2002 -- benchmarks used in Bolukbasi et al.\n",
    "\n",
    "%run 'evaluation/all_wordsim_jb.py' 'embeddings/' 'evaluation/data/word-sim/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MSR Analogies Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./evaluation/data/analogies.json', \"r\") as f:\n",
    "    analogies = json.load(f) # This is HUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('analogies_shorter.json', \"r\") as f:\n",
    "    analogies_shorter = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./evaluation/data/analogies_2000.json', \"r\") as f:\n",
    "    analogies_final = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_analogies(analogy_list,e):\n",
    "    num_analogies = 0\n",
    "    correct = 0  \n",
    "    num_missing = 0\n",
    "    #keep_analogies = []\n",
    "    t0 = time.time()\n",
    "    print_freq = 500\n",
    "\n",
    "    for i,wds in enumerate(analogy_list):\n",
    "        # we'll get KeyError if a word can't be found\n",
    "        #print(wds)\n",
    "        try:\n",
    "            \n",
    "            # generate \"d\" in a:b::c:d analogies, given a, b, and c\n",
    "            d = vm.show_analogy(e,wds[0],wds[1],wds[2],5)[0] # needs to be at least 2 \n",
    "\n",
    "            # keep track of how many analogies were computed\n",
    "            num_analogies += 1\n",
    "            #keep_analogies.append(wds)\n",
    "\n",
    "            if d == wds[3]:\n",
    "                correct += 1\n",
    "                \n",
    "        except:\n",
    "            #bad_analogies.append(wds)\n",
    "            num_missing += 1\n",
    "        \n",
    "        if i % print_freq  == 0:\n",
    "            print(\"Completed {:d} analogies in {:s}\".format(i, utils.pretty_timedelta(since=t0)))\n",
    "    \n",
    "    try: # if we're running on a subset, we may actually not be able to do _any_ analogies\n",
    "        score = correct / num_analogies\n",
    "    except:\n",
    "        score = 0\n",
    "                \n",
    "    print(\"Computed {:d}/{:d} analogies correctly in {:s}, accuracy: {:.2f}\".format(correct,num_analogies,utils.pretty_timedelta(since=t0),score))\n",
    "    \n",
    "    #return keep_analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 analogies in 0:00:00\n",
      "Completed 500 analogies in 0:03:58\n",
      "Completed 1000 analogies in 0:08:48\n",
      "Completed 1500 analogies in 0:23:30\n",
      "Computed 1379/2000 analogies correctly in 0:28:34, accuracy: 0.69\n"
     ]
    }
   ],
   "source": [
    "compare_analogies(analogies_final,E_gn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 analogies in 0:00:00\n",
      "Computed 62/100 analogies correctly in 0:00:47, accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "compare_analogies(analogies_final[:100],E_gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 0 analogies in 0:00:00\n",
      "Computed 61/100 analogies correctly in 0:00:48, accuracy: 0.61\n"
     ]
    }
   ],
   "source": [
    "compare_analogies(analogies_final[:100],E_gn_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intersect(l1, l2):\n",
    "    return [wds for wds in l1 if wds in l2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove(l1,l2):\n",
    "    for i,item in enumerate(l1):\n",
    "        l2.remove(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('analogies_shorter.json', \"w\") as f:\n",
    "    json.dump(short_copy,f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('analogies_2000.json', \"w\") as f:\n",
    "    json.dump(list_of_random_items,f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Select just 2000 random analogies\n",
    "import random\n",
    "\n",
    "list_of_random_items = random.sample(analogies_shorter, 2000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
